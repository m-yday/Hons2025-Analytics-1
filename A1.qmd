---
title: "Analytics"
subtitle: "Assignment 1"
authors:
  - name: Victoria Dingle
  - name: Shvet Maharaj
format: html
date: last-modified
execute:
  echo: false
  warning: false
  message: false
  cache: true
  freeze: auto
---

```{r}
#| label: "Libraries and data"
#| output: false

library(tidyverse)
library(futureverse)
library(glmnet)
library(glmnetUtils)
library(broom)
library(knitr)
library(kableExtra)
library(GGally)

train_data <- read.csv("online_shopping_train.csv", header = TRUE)

```

# Introduction

# Categorical variables
In our dataset we have some numerical variables and some categorical variables.
Our numerical variables are `Administrative`, `Administrative duration`, `Informational`, `Informational duration`, `Product related`, `Product related duration`, `Bounce rates`, `Exit rates`, `Page values` and `Special day`.
We have a number of categorical variables which will need to be converted into factors before any regression analysis can be applied to the dataset. These include `Month`, `OperatingSystems`, `Browser`, `VisitorType`, `Weekend`, and `Revenue` (our target variable). We will convert our categorical variables (except `Revenue`) into factors.

```{r}
#| label: "Categorical to factors"
#| output: hide

train_data <- train_data |> 
  mutate(
    Month = as.factor(Month), 
    OperatingSystems = as.factor(OperatingSystems), 
    Browser = as.factor(Browser), 
    VisitorType = as.factor(VisitorType), 
    Weekend = as.factor(Weekend))
```

We want to fit a linear regression model to our data. We begin with a saturated model.

```{r}
#| label: "Saturated model"
# First we standardise, because our variables have different scales.
train_x_stand <- train_data[, 1:10] |>
  scale()
train_stand <- data.frame(train_x_stand, train_data[,11:16])
lm_full <- glm(Revenue ~ ., data=train_stand, family=binomial)
lm_full |>
  tidy() |>
  kable(digits=2, caption="Saturated linear model of the online shopping data") 
```
Considering the coefficients of the greatest magnitude:

In order of the largest effects:

- Months:
With reference to August: 
Greatest increase: November +0.49
Greatest decrease: February -1.77

- PageValues +1.46

- VisitorType:
With reference to New Visitor:
Greatest decrease: Other -0.86


Considering the p-values obtained from a test of significance of the coefficient estimates:

Seemingly significant:
- Exit Rates 0.00
- Page Values 0.00
- Months with reference to August:
    December, March, May 0.00
    November 0.01
    February 0.03
- Visitor Types with reference to New Visitor:
    Returning Visitor 0.00
- Weekend with reference to False:
    True 0.06
- Informational 0.08
- Product Related Duration 0.08
- Browser with reference to 1:
    0.09

Seemingly insignificant:
- Months with reference to August:
    September 0.96
    October 0.62
    July 0.45
- Informational Duration 0.91
- Special Day 0.91
- Browser with reference to 1:
    2 0.89
    4 0.59
    10 0.55
    6 0.54
    3 0.45
- Operating Systems with reference to 1:
    2 0.87
- Administrative 0.69
- Visitor Type with reference to New:
    Other 0.44
- Administrative Duration 0.40



This model has many, many variables, considering the categorical variables each appear with their number of levels - 1 coefficients in it. Firstly, this is too many variables to be useful for interpretability, and secondly, we can see from the estimated coefficients (in conjunction with their standard errors and p-values) that many of these variables are not significant predictors of Revenue in our model.
We therefore wish to clean up our model such that is uses only significant predictors, so we now regularise this model using elasticnet. regression.

```{r}
#| label: "Elastic net regularisation"

elasticnet <- cva.glmnet(Revenue ~., train_stand, family=binomial, alpha = seq(0, 1, 0.1), nfolds=10)

{
  par(mfrow=c(1,1), pty="m", cex=0.6)
  plot(elasticnet)
  legend_labels <- lapply(elasticnet$alpha, function(x) format(x, digits = 3))
}

# df1 <- as.data.frame(matrix(cbind(coef(elasticnet, alpha=0), coef(elasticnet, alpha=0.1), coef(elasticnet, alpha=0.2), coef(elasticnet, alpha=0.3), coef(elasticnet, alpha=0.4), coef(elasticnet, alpha=0.5), coef(elasticnet, alpha=0.6), coef(elasticnet, alpha=0.7), coef(elasticnet, alpha=0.8), coef(elasticnet, alpha=0.9), coef(elasticnet, alpha=1)), ncol=11))
# colnames(df1) <- c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
# rownames(df1) <- rownames(coef(elasticnet, alpha=1))

# kable(df1, digits=3)
```

```{r}
#| label: "Min CV for elasticnet"

# extract cvm from each alpha tried in the elastic-net regression
# then find where precisely it came from
alpha_spot <- 11; cvmin = 1000; lambda_spot = -5;
for (i in 1:length(elasticnet$modlist))
{
  temp <- elasticnet$modlist[[i]]$cvm
  j <- min(temp)
  if (j < cvmin)
  {
    cvmin = j
    alpha_spot = i
    lambda_spot = which(temp == j)
  }
}
opt_alpha = elasticnet$alpha[alpha_spot]
opt_lambda = elasticnet$modlist[[alpha_spot]]$lambda[lambda_spot]
```

The optimal values are: `alpha` = `r opt_alpha`, `lambda` = `r opt_lambda`. The resulting minimum cross-validation error is `cvmin` = `r cvmin`. 

```{r}
# Make this one plot and put different colours in
minlossplot(elasticnet, ylim=c(0.586, 0.608), main="CV Loss plot using minimum CV loss", cv.type="min")
minlossplot(elasticnet, ylim=c(0.586, 0.608), main="CV Loss plot using CV loss within 1 standard error of the minimum", cv.type="1se")
```

Our elasticnet regularisation has selected an alpha value of 0.5, which corresponds to pure Lasso Regression. This makes sense because our original model had many variables, with few of them appearing significant in the saturated model. There is a large difference in CV MSE for $\alpha = 0$ and $\alpha \neq 0$ in both CV Loss plots (although we typically use the "within 1 standard error" measure), which can be understood, because applying pure ridge regression will not eliminate any variables from the model, but rather shrink their coefficients nearly to 0, which does not address the issue we raised earlier of having too many seemingly insignificant predictor variables. 

Thus we choose these values for alpha and lambda.

We choose something here, and we motivate it. In fact, that's pretty important. Please don't forget! (Especially because leaving this paragraph in will be awkward...)

Next, we use 

```{r}

```